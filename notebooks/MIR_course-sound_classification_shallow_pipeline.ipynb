{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> MUSIC INFORMATION RETRIEVAL</center>\n",
    "## <center>Sound classification - shallow pipeline</center>      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40019,
     "status": "ok",
     "timestamp": 1674571298108,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "sliVDC3GYPcT",
    "outputId": "c9a65daa-be4e-43fa-b598-84b2e819679b"
   },
   "outputs": [],
   "source": [
    "# install required packages\n",
    "!pip install essentia\n",
    "!pip install mirdata\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1255,
     "status": "ok",
     "timestamp": 1674571299360,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "8RP7NclOZ3BO"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import mirdata\n",
    "import essentia.standard as ess\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** *The following cell is needed to download a csv data file.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore sound classification addressing the task of [Mridangam](https://en.wikipedia.org/wiki/Mridangam) stroke type classification. \n",
    "\n",
    "**Note**: *this notebook is based on Marius Miron class materials and in a similar notebook available in*\n",
    "[github.com/MTG/MIRCourse](https://github.com/MTG/MIRCourse/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to run the notebook\n",
    "You can download the notebook and run it locally in your computer.\n",
    "\n",
    "You can also run it in Google Colab by using the following link.\n",
    "\n",
    "<table align=\"center\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/mrocamora/mir_course/blob/main/notebooks/MIR_course-sound_classification_shallow_pipeline.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rItYPqcEGLw_"
   },
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrQgPGt5GSZU"
   },
   "source": [
    "We initialize Mridangam stroke a collection of 7162 audio examples of individual strokes of the Mridangam in various tonics. The dataset comprises of 10 different strokes played on Mridangams with 6 different tonic values. \n",
    "\n",
    "In this experiment we predict 10 stroke classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3557,
     "status": "ok",
     "timestamp": 1674571302913,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "U-d7Tw0JGMlA"
   },
   "outputs": [],
   "source": [
    "mridangam = mirdata.initialize(\"mridangam_stroke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24672,
     "status": "ok",
     "timestamp": 1674571327582,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "i_y1fdZZZ3BZ",
    "outputId": "367aba73-330b-4949-abba-62231dfde158"
   },
   "outputs": [],
   "source": [
    "# This cell downloads and validates the mridangam dataset\n",
    "mridangam.download()  # download the dataset\n",
    "#mridangam_stroke.validate()  # validate that all the expected files are there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1674571327582,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "7qc4QShb2M71",
    "outputId": "2d2d819c-34b3-46e3-d44a-056df81fac8d"
   },
   "outputs": [],
   "source": [
    "mridangam_ids = mridangam.track_ids  # Load Mridangam IDs\n",
    "mridangam_data = mridangam.load_tracks()  # Load Mridangam data\n",
    "\n",
    "mridangam_data[mridangam_ids[0]]  # Visualize a single track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1674571327583,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "qqYgBYSoF6Zw",
    "outputId": "b6fe29c2-b8d6-432e-a799-18de82b16f69"
   },
   "outputs": [],
   "source": [
    "# Get complete list of different strokes\n",
    "stroke_names = []\n",
    "for i in mridangam_ids:\n",
    "    stroke_names.append(mridangam_data[i].stroke_name)\n",
    "stroke_names = np.unique(stroke_names)\n",
    "\n",
    "print(stroke_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1674571327583,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "AfXV-LblZ3Bj",
    "outputId": "f5395992-abad-4ebf-fbc4-be1113bd9cbe"
   },
   "outputs": [],
   "source": [
    "# You can create a dictionary using stroke type as keys\n",
    "stroke_dict = {item: [] for item in stroke_names}\n",
    "for i in mridangam_ids:\n",
    "    stroke_dict[mridangam_data[i].stroke_name].append(mridangam_data[i].audio_path)\n",
    "\n",
    "stroke_dict['cha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "executionInfo": {
     "elapsed": 2085,
     "status": "ok",
     "timestamp": 1674571329666,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "Vr2f3y2dZ3By",
    "outputId": "c305dc8b-9047-456a-f4ee-972e3ddf1d4c"
   },
   "outputs": [],
   "source": [
    "# Raw-data preprocess analysis parameters\n",
    "_, fs = mridangam_data[mridangam_ids[0]].audio\n",
    "\n",
    "num_strokes = len(stroke_dict.keys())\n",
    "print(\"Plot waveforms of random samples of each stroke type...\")\n",
    "plt.figure(1, figsize=(5 * num_strokes, 3))\n",
    "file_ind_inlist = 0 # 0: let's take the first file in the list for sample plots\n",
    "for i, stroke in enumerate(stroke_dict.keys()):\n",
    "    sample_file = stroke_dict[stroke][file_ind_inlist]\n",
    "    x = ess.MonoLoader(filename = sample_file, sampleRate = fs)()\n",
    "    \n",
    "    plt.subplot(1,num_strokes,(i+1))\n",
    "    plt.plot(x)\n",
    "    plt.title(stroke)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kceA0MUMGaMm"
   },
   "source": [
    "## Preprocessing of raw data: Segmentation, splitting, alignment,... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jT1ghfoGWJr"
   },
   "source": [
    "While common preprocessing steps (such as amplitude normalisation) exist, there is often some dataset and task specific preprocessing tasks required for grouping, cleaning, and format change. \n",
    "\n",
    "We would like to investigate the option of splitting the strokes using a fixed energy threshold (which would help us ignore the silence regions). Let's define a function to perform this operation and visualize some samples to observe the effectiveness of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1674571329667,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "Zxh3GIATiJzm"
   },
   "outputs": [],
   "source": [
    "# Raw-data preprocess analysis parameters\n",
    "windowSize = 1024\n",
    "hopSize = 512\n",
    "NRG_threshold_ratio = 0.005 #threshold expressed as ratio with respect to the maximum value\n",
    "#Let's put in a container to be able to use as a single argument in function calls\n",
    "params = {\"fs\":fs, \"windowSize\":windowSize, \"hopSize\":hopSize, \"NRG_threshold_ratio\": NRG_threshold_ratio}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1674571329667,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "amlqoQ_RiM7y"
   },
   "outputs": [],
   "source": [
    "#Function definition\n",
    "def split_file(filename, params):\n",
    "    '''Function to define split boundaries based on a fixed energy threshold\n",
    "    '''\n",
    "    x = ess.MonoLoader(filename = filename, sampleRate = fs)()\n",
    "    NRG = [];\n",
    "    #Main windowing and feature extraction loop\n",
    "    for frame in ess.FrameGenerator(x, frameSize = windowSize, hopSize = hopSize, startFromZero = True):\n",
    "        NRG.append(ess.Energy()(frame))\n",
    "    NRG = np.array(NRG)\n",
    "    NRG = NRG / np.max(NRG)\n",
    "    \n",
    "    #Applying energy threshold to decide wave split boundaries\n",
    "    split_decision_func = np.zeros_like(NRG)\n",
    "    split_decision_func[NRG > NRG_threshold_ratio] = 1\n",
    "    #Setting segment boundaries\n",
    "    #Inserting a zero at the beginning since we will decide the transitions using a diff function\n",
    "    split_decision_func = np.insert(split_decision_func, 0, 0)\n",
    "    diff_split_decision = np.diff(split_decision_func)\n",
    "    #Start indexes: transition from 0 to 1\n",
    "    start_indexes = np.nonzero(diff_split_decision > 0)[0] * hopSize\n",
    "    #Stop indexes: transition from 1 to 0\n",
    "    stop_indexes = np.nonzero(diff_split_decision < 0)[0] * hopSize\n",
    "    return (x, NRG, split_decision_func, start_indexes, stop_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "executionInfo": {
     "elapsed": 1826,
     "status": "ok",
     "timestamp": 1674571331490,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "47Akom3niaUD",
    "outputId": "a76f3f8a-a33f-43be-e297-56b330b98a3c"
   },
   "outputs": [],
   "source": [
    "num_strokes = len(stroke_names)\n",
    "print(\"Sample plots for waveform versus energy and splits based on energy threshold\")\n",
    "\n",
    "file_indexes = [1]\n",
    "for file_ind_inlist in file_indexes:\n",
    "    plt.figure(file_ind_inlist, figsize=(5 * num_strokes, 3))\n",
    "    for i, stroke in enumerate(stroke_dict.keys()):\n",
    "        sample_file = stroke_dict[stroke][file_ind_inlist]\n",
    "        (x, NRG, split_decision_func, start_indexes, stop_indexes) = split_file(sample_file, params)\n",
    "        #Plotting functions for checking the split decisions\n",
    "        plt.subplot(1,num_strokes,(i+1))\n",
    "        plt.title(stroke)\n",
    "        plt.plot(x, label = 'sound waveform')\n",
    "        plt.plot(np.arange(NRG.size) * hopSize, NRG, 'g', label = 'NRG')\n",
    "        plt.plot(np.arange(split_decision_func.size) * hopSize, split_decision_func,'r', label = 'split function')\n",
    "        plt.vlines(start_indexes, ymin = -0.5, ymax = 0, colors='b', linestyles='solid', label='Segment start')\n",
    "        plt.vlines(stop_indexes, ymin = -0.5, ymax = 0, colors='k', linestyles='dashed', label='Segment stop')\n",
    "\n",
    "plt.legend(loc=\"best\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51433,
     "status": "ok",
     "timestamp": 1674571382920,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "FGIUqV_xk2v7",
    "outputId": "58b47f74-0ba3-491e-c23c-05bd53a1298f"
   },
   "outputs": [],
   "source": [
    "main_data_dir = 'sample_data'\n",
    "if not os.path.exists(main_data_dir): #creating the directory\n",
    "    os.mkdir(main_data_dir)\n",
    "segments_dir = os.path.join(main_data_dir,'segments')\n",
    "if not os.path.exists(segments_dir): #creating the directory\n",
    "    os.mkdir(segments_dir)\n",
    "\n",
    "segment_files = []\n",
    "for stroke, files in stroke_dict.items():\n",
    "    for sample_file in files:\n",
    "        #Get file id\n",
    "        stroke_id =  sample_file.split('__')[-1].split('.')[0]\n",
    "        x = ess.MonoLoader(filename = sample_file, sampleRate = fs)()\n",
    "        (x, NRG, split_decision_func, start_indexes, stop_indexes) = split_file(sample_file, params)\n",
    "        #Croping segments\n",
    "        for start, stop in zip(start_indexes, stop_indexes):\n",
    "            x_seg = x[start: stop]\n",
    "            #Final check for amplitude (to avoid silent segments selection due to noise in split function)\n",
    "            if(np.max(np.abs(x_seg)) > 0.05):\n",
    "                #Amplitude normalisation\n",
    "                x_seg = x_seg / np.max(np.abs(x_seg))\n",
    "                filename = os.path.join(segments_dir, stroke_id + '.wav')\n",
    "                ess.MonoWriter(filename = filename, format = 'wav', sampleRate = fs)(x_seg)\n",
    "                segment_files.append(filename)\n",
    "\n",
    "print(len(segment_files),'segment files created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 336,
     "status": "ok",
     "timestamp": 1674571383248,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "JQkOjXfTnJdi"
   },
   "outputs": [],
   "source": [
    "#If required, you can use this cell to delete all files in a given folder\n",
    "def delete_files_in_dir(dir_name):\n",
    "    '''Deleting all files in a directory\n",
    "    '''\n",
    "    for root, dirs, files in os.walk(dir_name):\n",
    "        for file in files:\n",
    "            file_name = os.path.join(root,file)\n",
    "            os.remove(file_name);print(file_name, 'removed');\n",
    "\n",
    "#delete_files_in_dir(segments_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmTShfNLZ3Cg"
   },
   "source": [
    "## Feature extraction \n",
    "\n",
    "Let's compute a list of common features for each of the files and form a data frame including features and categories. We will be using the [MusicExtractor function of Essentia](https://essentia.upf.edu/documentation/reference/std_MusicExtractor.html) that would compute a large number of features commonly used in MIR literature. \n",
    "\n",
    "Essentia-MusicExtractor extracts a large number of features. For simplicity, let's only keep low-level descriptors which are represented with a single scalar value as our feature set and discard other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1674571383248,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "TvOzjojcZ3Co",
    "outputId": "9a92622c-fa00-417a-9e48-65821bdc5856"
   },
   "outputs": [],
   "source": [
    "# Print descriptors to be considered\n",
    "features, features_frames = ess.MusicExtractor(lowlevelSilentFrames='drop',\n",
    "                                                      lowlevelFrameSize = 2048,\n",
    "                                                      lowlevelHopSize = 1024,\n",
    "                                                      lowlevelStats = ['mean', 'stdev'])(mridangam_data['224030'].audio_path)\n",
    "\n",
    "scalar_lowlevel_descriptors = [descriptor for descriptor in features.descriptorNames() if 'lowlevel' in descriptor and isinstance(features[descriptor], float)]\n",
    "print(\"Subset of features to be considered:\\n\",scalar_lowlevel_descriptors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"> WARNING: </span> the following process can take some time. You may download pre-computed features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5aBdpBtZ3Cv"
   },
   "source": [
    "Running musicextractor for all files, keeping a subset of features, writing to an output file: data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 935167,
     "status": "ok",
     "timestamp": 1674572318414,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "POoJ0K8BZ3Cz",
    "outputId": "973d89a0-6b5e-48ba-f732-868e7786334b"
   },
   "outputs": [],
   "source": [
    "# Extracting features and writing in data.csv file in the segments folder\n",
    "#  each line in the data.csv file represents a sample with features and the class information as the last element\n",
    "data_file = 'mridangam_stroke_features.csv'\n",
    "file_count = 0\n",
    "with open(data_file, 'w') as writer:\n",
    "    #adding column names as the first line in csv\n",
    "    line2write = ','.join(scalar_lowlevel_descriptors + ['stroke']).replace('lowlevel.','') + '\\n'\n",
    "    writer.write(line2write)\n",
    "    for filename in segment_files:\n",
    "        file_count +=1\n",
    "        if file_count % 20 == 0: #print name of a file every 20 files\n",
    "            print(file_count, \"files processed, current file: \", filename)\n",
    "\n",
    "        #Compute and write features for file\n",
    "        features, features_frames = ess.MusicExtractor(lowlevelSilentFrames='drop',\n",
    "                                                      lowlevelFrameSize = 2048,\n",
    "                                                      lowlevelHopSize = 1024,\n",
    "                                                      lowlevelStats = ['mean', 'stdev'])(filename)\n",
    "        selected_features = [features[descriptor] for descriptor in scalar_lowlevel_descriptors]\n",
    "        label = filename.split('/')[-1].split('.')[0].split('-')[0]\n",
    "        line2write = str(selected_features)[1:-1] + ',' + label + '\\n'\n",
    "        writer.write(line2write)\n",
    "print(\"A total of \", file_count, \"files processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the two following lines to download the pre-computed features\n",
    "# data_file = 'mridangam_stroke_features.csv'\n",
    "# wget.download('https://github.com/mrocamora/mir_course/blob/main/data/mridangam_stroke_features.csv?raw=true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "executionInfo": {
     "elapsed": 350,
     "status": "ok",
     "timestamp": 1674572318763,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "CaJDBafwZ3C-",
    "outputId": "be7e1874-4e67-49bb-9e99-783af42ecbe4"
   },
   "outputs": [],
   "source": [
    "#Read data with pandas module\n",
    "data = pd.read_csv(data_file)\n",
    "\n",
    "#Plot first lines of our data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "executionInfo": {
     "elapsed": 1668,
     "status": "ok",
     "timestamp": 1674572320428,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "_zrFi7BLMhai",
    "outputId": "e2e07a2f-3fe8-42e3-e92d-20d4cd957215"
   },
   "outputs": [],
   "source": [
    "# Take two random features and plot the sample points\n",
    "import seaborn as sns\n",
    "sns.relplot(x = \"melbands_flatness_db.mean\", y = \"spectral_centroid.mean\", hue = \"stroke\", data = data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dU9u2YaDZ3DV"
   },
   "source": [
    "### Preprocessing of the features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1674572320429,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "amcLwVPkZ3DX"
   },
   "outputs": [],
   "source": [
    "data_modif = data.copy()\n",
    "\n",
    "#Let's use sklearn's preprocessing tools for applying normalisation to features\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "data_modif.iloc[:,:84] = min_max_scaler.fit_transform(data.iloc[:,:84].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1674572320429,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "hByJk6RnZ3Dh",
    "outputId": "3c115f2b-bb81-4860-eeb5-5a4f4cc4819a"
   },
   "outputs": [],
   "source": [
    "# Checking if our data is balanced (if not, we should balance it to prevent our model to be baised)\n",
    "data_modif.stroke.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1674572320429,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "vBrUNGyqZ3Dq",
    "outputId": "092914f4-a503-4306-ebfb-fb8b0c08351e"
   },
   "outputs": [],
   "source": [
    "# Here we don't pick the lowest number, 46 for bheem, which is too small. Instead, we choose the 330 of \"cha\"\n",
    "min_number = data_modif.stroke.value_counts()['cha']\n",
    "thi_data = data_modif[data_modif.stroke == 'thi'].sample(n = min_number, random_state = 42)\n",
    "tha_data = data_modif[data_modif.stroke == 'tha'].sample(n = min_number)\n",
    "ta_data = data_modif[data_modif.stroke == 'ta'].sample(n = min_number)\n",
    "thom_data = data_modif[data_modif.stroke == 'thom'].sample(n = min_number)\n",
    "num_data = data_modif[data_modif.stroke == 'num'].sample(n = min_number)\n",
    "dhin_data = data_modif[data_modif.stroke == 'dhin'].sample(n = min_number)\n",
    "dheem_data = data_modif[data_modif.stroke == 'dheem'].sample(n = min_number)\n",
    "tham_data = data_modif[data_modif.stroke == 'tham'].sample(n = min_number)\n",
    "cha_data = data_modif[data_modif.stroke == 'cha'].sample(n = min_number)\n",
    "bheem_data = data_modif[data_modif.stroke == 'bheem']\n",
    "#Merging after downsampling\n",
    "data_modif = pd.concat([thi_data, tha_data, ta_data, thom_data, num_data, dhin_data, dheem_data, tham_data, cha_data, bheem_data])\n",
    "#Checking the balance again\n",
    "data_modif.stroke.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6X3nwzjCIez"
   },
   "source": [
    "## Supervised learning approach (Support Vector Machines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1674572320429,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "agdVdsE6Z3Dz",
    "outputId": "e66f8bb1-dd11-47b5-c9f0-02dc64962a2b"
   },
   "outputs": [],
   "source": [
    "# Input values put in a matrix, there are 84 features\n",
    "X = data_modif.iloc[:,:84].values \n",
    "# Creating output values\n",
    "data_modif.stroke = pd.Categorical(data_modif.stroke)  # convert to categorical data\n",
    "y = np.array(data_modif.stroke.cat.codes)  # create label encoded outputs\n",
    "# Print the first sample\n",
    "print(\"Features of the first sample: \", X[0])\n",
    "print(\"Class of the first sample: \", y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1674572320429,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "CHS_06ejZ3D-",
    "outputId": "85bf9c57-2cfd-48d1-cb23-ae6e116b2728"
   },
   "outputs": [],
   "source": [
    "# Let's split data into test and train sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)\n",
    "\n",
    "# Check sizes of input and output vectors\n",
    "print(\"Size of train features matrix: \",X_train.shape, \", Size of train output vector: \",y_train.shape)\n",
    "print(\"Size of test features matrix: \",X_test.shape, \", Size of test output vector: \",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1674572320429,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "RsoR_KCwZ3EG"
   },
   "outputs": [],
   "source": [
    "# Define and train the model\n",
    "\n",
    "from sklearn import svm\n",
    "clf = svm.SVC(gamma = 1 / (X_train.shape[-1] * X_train.var()))\n",
    "\n",
    "# Fit model with training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict classes of test samples\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1674572320430,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "ncsEtE4IZ3EN",
    "outputId": "f8e8cbc1-7e09-438f-eb02-a2b136e51825"
   },
   "outputs": [],
   "source": [
    "# Test the model\n",
    "# Let's check for each sample in the test set if prediction matches the true class information\n",
    "y_test == y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1674572320430,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "p54ZOy8QZ3EP",
    "outputId": "5cc1bb31-af7c-4650-86b4-438e05c0a793"
   },
   "outputs": [],
   "source": [
    "# Data is balanced, so you can use accuracy as a measure:\n",
    "print(\"accuracy: \", np.sum(y_test == y_pred)/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 379
    },
    "executionInfo": {
     "elapsed": 604,
     "status": "ok",
     "timestamp": 1674572321030,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "0SfE74fYZ3EU",
    "outputId": "9188fee5-e1df-41ec-b8d2-9dc7588afe03"
   },
   "outputs": [],
   "source": [
    "# Print the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "classes = np.unique(data_modif.stroke)\n",
    "conf_mat = pd.DataFrame(confusion_matrix(y_test, y_pred), columns = classes, index = classes)\n",
    "conf_mat.index.name = 'Actual'\n",
    "conf_mat.columns.name = 'Predicted'\n",
    "plt.figure(figsize = (7, 5))\n",
    "sns.set(font_scale = 1.2)\n",
    "sns.heatmap(conf_mat, cmap = \"Blues\", annot_kws = {\"size\": 12}, annot = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pPbV0-Krdtl"
   },
   "source": [
    "## Supervised learning approach (Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 327,
     "status": "ok",
     "timestamp": 1674572321354,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "8jENK7AuH-92"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Let's start by splitting our data \n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size = 0.20, random_state = 1)\n",
    "\n",
    "# ..and apply normalisation\n",
    "scaler = StandardScaler().fit(X_train_val)\n",
    "norm_x_train_val = scaler.transform(X_train_val)\n",
    "norm_x_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1674572321354,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "2LQQDNeRH-9-",
    "outputId": "f0901c99-5809-4e53-8772-e6173cb706f7"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train_val_onehot = onehot_encoder.fit_transform(y_train_val.reshape(len(y_train_val), 1))\n",
    "y_test_onehot = onehot_encoder.fit_transform(y_test.reshape(len(y_test), 1))\n",
    "print(\"One-hot encoded y_train_val shape = \", y_train_val_onehot.shape)\n",
    "print(\"One-hot encoded y_test shape = \", y_test_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2881,
     "status": "ok",
     "timestamp": 1674572324233,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "DwoS-ycaH--A"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "def compose_model(num_features):\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    # Input layer \n",
    "    model.add(layers.BatchNormalization(name='InputLayer', input_shape=(num_features,)))\n",
    "    \n",
    "    # 1. hidden layer\n",
    "    model.add(layers.Dense(name='HiddenLayer_1', units = 40))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # 2. hidden layer\n",
    "    model.add(layers.Dense(name='HiddenLayer_2', units = 20))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(layers.Dense(name='Output_layer', units = 10))\n",
    "    model.add(layers.Activation('sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = compose_model(X_train_val.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1674572324234,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "NEdSFyGjNook",
    "outputId": "cbf9e5f3-a09c-42db-feff-ba844892912c"
   },
   "outputs": [],
   "source": [
    "model.summary()  # Plot summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 43257,
     "status": "ok",
     "timestamp": 1674572367489,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "5K_wFcbXH--F"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "summary = model.fit(X_train_val, y_train_val_onehot, batch_size = 50, epochs = 250, validation_split=0.2, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1674572367490,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "-CBE3EHiH--I",
    "outputId": "52186dcc-9ade-4932-e9f9-d21ba7ffd769"
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test_onehot, verbose = 0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1674572367490,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "3Aa26zN7H--M",
    "outputId": "17807817-e196-4ffd-8048-631a22ff03f8"
   },
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(summary.history['accuracy'])\n",
    "plt.plot(summary.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='lower right')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(summary.history['loss'])\n",
    "plt.plot(summary.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
