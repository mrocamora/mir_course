{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> MUSIC INFORMATION RETRIEVAL</center>\n",
    "## <center> Audio fingerprinting - with audfprint</center>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: *this notebook its based on the one prepared by **Marius Miron** for the MIR course.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebooks uses `audfprint` by Dan Ellis, which can take a list of soundfiles and create a database of landmarks, and then subsequently take one or more query audio files and match them against the previously-created database. This can be used e.g. to \"de-duplicate\" a collection of music. The fingerprint is robust to things like time skews, different encoding schemes, and even added noise. \n",
    "\n",
    "We also use `mirdata` to manage a dataset of audio recordings and `audiomentations` for audio transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to run the notebook\n",
    "You can download the notebook and run it locally in your computer.\n",
    "\n",
    "You can also run it in Google Colab by using the following link.\n",
    "\n",
    "<table align=\"center\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/mrocamora/mir_course/blob/main/notebooks/MIR_course-audio_fingerprinting_with_audfprint.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lm9efOakI1li"
   },
   "source": [
    "### Installation of packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTTsPvdgI7T0"
   },
   "source": [
    "We install mirdata to manage the datasets and audiomentations for audio transformations such as adding noise, time stretching, pitch shifting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dCufFo8CkGry",
    "outputId": "83e9519d-fad0-4c4b-e774-1b55dd58be5c"
   },
   "outputs": [],
   "source": [
    "!pip install mirdata\n",
    "!pip install audiomentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OjnuFEwJLPm"
   },
   "source": [
    "We download the audfprint repository which has an audio fingerprint API which can construct a database from a directory and query the database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wMrGwWiZhuNM",
    "outputId": "bc72a0d7-7ba7-48c5-8231-1dd45af962ce"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/dpwe/audfprint.git\n",
    "%cd audfprint/\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUeFuBmkJ6X6"
   },
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0aL6pWoJWod"
   },
   "source": [
    "We import the libraries and we initialize and download the Orchset dataset with mirdata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ElJQ2pIbjF01"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import mirdata\n",
    "import soundfile as sf\n",
    "import audiomentations\n",
    "dataset = mirdata.initialize(\"orchset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YSb7Z2y3kvEH",
    "outputId": "8606ee73-bc14-4db1-f762-b0589866688c"
   },
   "outputs": [],
   "source": [
    "dataset.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "-yurv0W8K5nP",
    "outputId": "178ed49f-7820-491e-8c3a-250418a17663"
   },
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "tracks = dataset.load_tracks()  \n",
    "track = tracks['Beethoven-S5-I-ex1']\n",
    "x, sr = track.audio_mono\n",
    "ipd.Audio(x,rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9v86ppqXKBGF"
   },
   "source": [
    "### Fingerprinting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKGawUuMJfef"
   },
   "source": [
    "This is the path where the mono audio files are located. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "rdMkgCoGk4z_",
    "outputId": "42d16789-6c48-41cd-f040-92cf5a1df8ce"
   },
   "outputs": [],
   "source": [
    "os.path.join(dataset.data_home,'audio','mono')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BzZAPXvMJk8G"
   },
   "source": [
    "Using the path above we can construct a new database by ingesting all the files in the corresponding path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I7lJnZTZpBmF",
    "outputId": "7f34a637-587a-4f51-8702-21d84f2d438a"
   },
   "outputs": [],
   "source": [
    "!python audfprint.py new --dbase fpdbase.pklz /root/mir_datasets/orchset/audio/mono/*.wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5ujicNxJ01c"
   },
   "source": [
    "We test the system with the stereo audio for one track in the database. The system should be robust to taking one of the channels as input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qiggCwMX6FP4",
    "outputId": "4c91013f-d012-4ab3-a424-6e2f551bef95"
   },
   "outputs": [],
   "source": [
    "!mkdir '/root/temp'\n",
    "x, sr = track.audio_stereo\n",
    "out_filename = os.path.join('/root/temp',track.track_id+'-L.wav')\n",
    "print(out_filename)\n",
    "sf.write(out_filename, x[0,int(5*sr):], sr)\n",
    "out_filename = os.path.join('/root/temp',track.track_id+'-R.wav')\n",
    "print(out_filename)\n",
    "sf.write(out_filename, x[1,int(5*sr):], sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "btjxSMQR_1Ki",
    "outputId": "79f51d4a-f384-47c7-f221-1cfeec062c44"
   },
   "outputs": [],
   "source": [
    "!python audfprint.py match --dbase fpdbase.pklz /root/temp/Beethoven-S5-I-ex1-L.wav \n",
    "%matplotlib\n",
    "!python audfprint.py match --dbase fpdbase.pklz /root/temp/Beethoven-S5-I-ex1-R.wav -I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fy9_eJ3bImUH"
   },
   "source": [
    "The two queries contains audio from Beethoven-S5-I-ex1.wav starting at 5 sec into the track, left and right channel separately. There were a total of 61/43 landmark hashes shared between the query and that track. Generally, anything more than 5 or 6 consistently-timed matching hashes indicate a true match, and random chance will result in fewer than 1% of the raw common hashes being temporally consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3MddE5-rKWz_"
   },
   "source": [
    "The system should also be robust to various transformations like noise, time stretching, pitch shifting. We use audiomentations to compose a transformation by chaining up different audio effects. This is applied to a given audio track and written to the disk. We use audfprint to identify this file in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180
    },
    "id": "wO9gnrKVDEVs",
    "outputId": "8daa4371-de34-43e0-b687-475dd0264fa4"
   },
   "outputs": [],
   "source": [
    "augment0 = audiomentations.Compose([\n",
    "    audiomentations.AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
    "    audiomentations.TimeStretch(min_rate=0.8, max_rate=1.25, p=0.5),\n",
    "    audiomentations.PitchShift(min_semitones=-4, max_semitones=4, p=0.5),\n",
    "    #audiomentations.Shift(min_fraction=-0.5, max_fraction=0.5, p=0.5),\n",
    "])\n",
    "x, sr = track.audio_mono\n",
    "# Augment/transform/perturb the audio data\n",
    "augmented_samples = augment0(samples=x, sample_rate=sr)\n",
    "out_filename = os.path.join('/root/temp',track.track_id+'-aug0.wav')\n",
    "print(out_filename)\n",
    "sf.write(out_filename, augmented_samples, sr)\n",
    "!python audfprint.py match --dbase fpdbase.pklz /root/temp/Beethoven-S5-I-ex1-aug0.wav\n",
    "ipd.Audio(augmented_samples,rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ev3G2t1ZKs2e"
   },
   "source": [
    "We can stress-test the system by increasing gradually the amount of noise until the song is not identified.\n",
    "\n",
    "What is the impact of these transformation on the methods using landmarks/peaks such as audfprint? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180
    },
    "id": "DqG9tzv4G7k-",
    "outputId": "2ce3529c-e74e-41db-e1ac-aa708bf11156"
   },
   "outputs": [],
   "source": [
    "augment1 = audiomentations.Compose([\n",
    "    audiomentations.AddGaussianNoise(min_amplitude=0.009, max_amplitude=0.09, p=0.99),\n",
    "    #audiomentations.AddShortNoises()\n",
    "])\n",
    "x, sr = track.audio_mono\n",
    "# Augment/transform/perturb the audio data\n",
    "augmented_samples = augment1(samples=x[int(5*sr):], sample_rate=sr)\n",
    "out_filename = os.path.join('/root/temp',track.track_id+'-aug1.wav')\n",
    "print(out_filename)\n",
    "sf.write(out_filename, augmented_samples, sr)\n",
    "!python audfprint.py match --dbase fpdbase.pklz /root/temp/Beethoven-S5-I-ex1-aug1.wav\n",
    "ipd.Audio(augmented_samples,rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "peWhJikMK7hz"
   },
   "source": [
    "\n",
    "Adding transformations to the original audio leads to a loss of peaks and to a change in distance between the pairs/triplets of peaks which generate different hashes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180
    },
    "id": "yAhdQ4VH9547",
    "outputId": "aca63cc8-d094-4aef-e09f-a89ae6220977"
   },
   "outputs": [],
   "source": [
    "augment2 = audiomentations.Compose([\n",
    "    audiomentations.AddGaussianNoise(min_amplitude=0.01, max_amplitude=0.1, p=0.99),\n",
    "    #audiomentations.AddShortNoises()\n",
    "])\n",
    "x, sr = track.audio_mono\n",
    "# Augment/transform/perturb the audio data\n",
    "augmented_samples = augment2(samples=x[int(5*sr):], sample_rate=sr)\n",
    "out_filename = os.path.join('/root/temp',track.track_id+'-aug2.wav')\n",
    "print(out_filename)\n",
    "sf.write(out_filename, augmented_samples, sr)\n",
    "!python audfprint.py match --dbase fpdbase.pklz /root/temp/Beethoven-S5-I-ex1-aug2.wav\n",
    "ipd.Audio(augmented_samples,rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VvjNHQawLUMO"
   },
   "source": [
    "Let's take a look under the hood to see how audfprint works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "Pl_H6YbbPxIf",
    "outputId": "26f00d17-b680-46c3-aab9-4cba38c88450"
   },
   "outputs": [],
   "source": [
    "import audfprint_analyze\n",
    "import audfprint_match\n",
    "import hash_table\n",
    "from audfprint_analyze import g2h_analyzer\n",
    "\n",
    "matcher = audfprint_match.Matcher()\n",
    "#from audfprint_analyze import Matcher\n",
    "\n",
    "pat = '/root/mir_datasets/orchset/audio/mono/*wav'\n",
    "qry = '/root/temp/Beethoven-S5-I-ex1-aug2.wav'\n",
    "\n",
    "####hash_tab = audfprint_analyze.glob2hashtable(pat) #new database\n",
    "hash_tab = hash_table.HashTable('fpdbase.pklz') #load old database\n",
    "\n",
    "g2h_analyzer = audfprint_analyze.Analyzer(density=40.0)\n",
    "\n",
    "rslts, dur, nhash = matcher.match_file(g2h_analyzer,hash_tab, qry)\n",
    "t_hop = 0.02322\n",
    "if len(rslts)>0:\n",
    "  print(\"Matched\", qry, \"(\", dur, \"s,\", nhash, \"hashes)\",\n",
    "          \"as\", hash_tab.names[rslts[0][0]],\n",
    "          \"at\", t_hop * float(rslts[0][2]), \"with\", rslts[0][1],\n",
    "          \"of\", rslts[0][3], \"hashes\")\n",
    "  %matplotlib inline\n",
    "  matcher.illustrate_match(g2h_analyzer, hash_tab, qry)\n",
    "else:\n",
    "  print(\"No matches\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MIRclass_fingerprinting_audfprint.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
