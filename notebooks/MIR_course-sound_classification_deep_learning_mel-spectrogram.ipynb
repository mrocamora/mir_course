{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"display: flex; align-items: center; justify-content: space-between;\">\n",
    "  Music Information Retrieval\n",
    "  <img src=\"https://www.upf.edu/documents/8071534/8177261/MTG_logo-07.png/c5d8ed89-90cf-7fbb-6116-1ab41706aa85?t=1578480743718\"\n",
    "       alt=\"MTG logo\"\n",
    "       width=\"180\"\n",
    "       style=\"vertical-align: middle; margin-left: 20px;\" />\n",
    "</h1>\n",
    "\n",
    "This notebook is part of the **Music Information Retrieval (MIR)** course within the  \n",
    "[**Sound and Music Computing (SMC)**](https://www.upf.edu/web/smc) programme  \n",
    "at the **Music Technology Group (MTG), Universitat Pompeu Fabra**.\n",
    "\n",
    "---\n",
    "\n",
    "### How to Run the Notebook\n",
    "You can download the notebook and run it locally on your computer.\n",
    "\n",
    "Alternatively, to access some GPU time for free, open it directly in **Google Colab** using the badge below:\n",
    "\n",
    "<a target=\"_blank\"\n",
    "   href=\"https://colab.research.google.com/github/mrocamora/mir_course/blob/main/notebooks/MIR_course-sound_classification_deep_learning_mel-spectrogram.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\"\n",
    "       alt=\"Open in Colab\" />\n",
    "</a>\n",
    "\n",
    "---\n",
    "\n",
    "# Sound classification - Deep-learning Pipeline\n",
    "## End-to-end classification using deep learning on the mel-spectrogram\n",
    "\n",
    "### About\n",
    "\n",
    "We will explore sound classification addressing the task of [Mridangam](https://en.wikipedia.org/wiki/Mridangam) stroke type classification. To do that, we will apply a deep-learning approach from the **mel-spectrogram** representation. \n",
    "\n",
    "**Note**: *this notebook is based on Marius Miron class materials.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cSJcgQwIrIq"
   },
   "source": [
    "## Installation of packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GugsmRYH_72"
   },
   "source": [
    "First of all we need to find out if the notebook is run on Colab and, if so, what version of cuda we have on the server. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1673437826222,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "Iidt0GzvHHxO",
    "outputId": "720ad2ae-1b9c-49c0-e449-f14ece6415bd"
   },
   "outputs": [],
   "source": [
    "RunningInCOLAB = 'google.colab' in str(get_ipython())\n",
    "if RunningInCOLAB:\n",
    "  !nvcc -V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFs_ZDemz3Vf"
   },
   "source": [
    "Then we install the pytorch version for the corresponding cuda version and the other packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27040,
     "status": "ok",
     "timestamp": 1673437853674,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "R4ePrUY2yoCh",
    "outputId": "b03cc670-19ac-4567-d3ae-87cf89aace47"
   },
   "outputs": [],
   "source": [
    "!pip install torch torchaudio\n",
    "!pip install lightning\n",
    "!pip install mirdata scikit-learn\n",
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBrIMBt6Jm34"
   },
   "source": [
    "We import the packages and we set the random_seed for our experiments. The random seed makes sure the experiment is reproducible on this environment.\n",
    "\n",
    "We use mirdata to load the datasets, sklearn for data partitioning, torchaudio to load and transform audio files, and pytorch lightning on top of pytorch for machine learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4649,
     "status": "ok",
     "timestamp": 1673437858316,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "pK5QL1yMHFmx",
    "outputId": "2041ac77-5668-4fee-c139-47b54057c7ef"
   },
   "outputs": [],
   "source": [
    "import mirdata\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import torch\n",
    "import torchaudio\n",
    "import lightning.pytorch as pl\n",
    "import torchmetrics\n",
    "\n",
    "RANDOM_SEED=0\n",
    "from lightning.pytorch import seed_everything\n",
    "seed_everything(seed=RANDOM_SEED, workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pokcYI_tK2rO"
   },
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lkgW6tvrKeaL"
   },
   "source": [
    "We initialize Mridangam stroke a collection of 6977 audio examples of individual strokes of the Mridangam in various tonics. The dataset comprises of 10 different strokes played on the Mridangam with 6 different tonic values.\n",
    "\n",
    "In this experiment we predict 10 stroke classes. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1893,
     "status": "ok",
     "timestamp": 1673437860201,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "7y6vQl5UHOkV"
   },
   "outputs": [],
   "source": [
    "mridangam = mirdata.initialize(\"mridangam_stroke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvCFJiXFKhYk"
   },
   "source": [
    "First time the dataset needs to be downloaded. This is fairly easy with the public datasets in mirdata, by calling the download method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31363,
     "status": "ok",
     "timestamp": 1673437891558,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "qSG57fP2gqDI",
    "outputId": "db3b6723-036b-4d9e-880f-f2f56a751ab1"
   },
   "outputs": [],
   "source": [
    "mridangam.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1029,
     "status": "ok",
     "timestamp": 1673437892577,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "-yurv0W8K5nP",
    "outputId": "bfa18055-7d1a-4683-f0bb-a40ba2bb64d7"
   },
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "mridangam.validate()  # validate dataset\n",
    "track = mridangam.choice_track()  # load a random track\n",
    "x, sr = track.audio\n",
    "ipd.Audio(track.audio_path)\n",
    "print(track)  # see what data a track contains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzWTTqxmKtwE"
   },
   "source": [
    "\n",
    "In order to use this dataset with pytorch, we extend the Dataset object to load the audio and annotations in our dataset, according to these [instructions](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html).\n",
    "\n",
    "We basically need to write three methods:\n",
    "\n",
    "\n",
    "*   __init__\n",
    "*   __len__\n",
    "*   __getitem__ to return each pair of audio array and class label\n",
    "\n",
    "For a good introduction to dataset loaders in the context of MIR see this tutorial [Deep Learning 101 for Audio based MIR](https://geoffroypeeters.github.io/deeplearning-101-audiomir_book/intro_pytorch.html)\n",
    "\n",
    "This is how a prototype of this class could look like:\n",
    "\n",
    "```\n",
    "class MridangamDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "      self.track_ids = dataset.track_ids\n",
    "    def __getitem__(self, index):\n",
    "      # load data\n",
    "      audio = load_audio(self.track_ids[index])\n",
    "      label = self.track_ids[index].label\n",
    "      # split audio in a fixed size array\n",
    "      audio = audio[:seq_duration] \n",
    "      return audio,label\n",
    "    def __len__(self):\n",
    "      return len(self.tracks_ids)\n",
    "\n",
    "```\n",
    "\n",
    "Let's implement the class:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1673437892578,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "irATuWrpHblx"
   },
   "outputs": [],
   "source": [
    "class MridangamDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        mirdataset,\n",
    "        seq_duration=0.5,\n",
    "        resample=8000,\n",
    "        subset=0,  # 0: train, 1: val, 2: test\n",
    "        val_split=0.1,\n",
    "        test_split=0.1,\n",
    "        random_seed=RANDOM_SEED\n",
    "    ):\n",
    "        self.seq_duration = seq_duration\n",
    "        self.dataset = mirdataset\n",
    "        self.track_ids = self.dataset.track_ids\n",
    "        self.tracks = self.dataset.load_tracks()\n",
    "        self.resample_rate = resample\n",
    "        self.set = subset\n",
    "\n",
    "        # --- Label setup ---\n",
    "        labels = [self.dataset.track(i).stroke_name for i in self.track_ids]\n",
    "        unique_labels = sorted(set(labels))  # sorted for reproducibility\n",
    "        self.label_map = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "\n",
    "        # --- Split dataset into train/val/test ---\n",
    "        trainval_ids, test_ids = sklearn.model_selection.train_test_split(\n",
    "            self.track_ids,\n",
    "            test_size=test_split,\n",
    "            random_state=random_seed,\n",
    "            stratify=labels\n",
    "        )\n",
    "\n",
    "        trainval_labels = [lbl for lbl, tid in zip(labels, self.track_ids) if tid in trainval_ids]\n",
    "        val_frac = val_split / (1 - test_split)\n",
    "\n",
    "        train_ids, val_ids = sklearn.model_selection.train_test_split(\n",
    "            trainval_ids,\n",
    "            test_size=val_frac,\n",
    "            random_state=random_seed,\n",
    "            stratify=trainval_labels\n",
    "        )\n",
    "\n",
    "        self.splits = {\n",
    "            0: train_ids,\n",
    "            1: val_ids,\n",
    "            2: test_ids\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        track_id = self.splits[self.set][index]\n",
    "        track = self.dataset.track(track_id)\n",
    "\n",
    "        # --- Load full signal ---\n",
    "        audio_signal, sample_rate = torchaudio.load(track.audio_path)\n",
    "        _, total_samples = audio_signal.shape\n",
    "\n",
    "        # --- Extract segment from start ---\n",
    "        num_frames = min(int(np.floor(self.seq_duration * sample_rate)), total_samples)\n",
    "        audio_segment = audio_signal[:, :num_frames]\n",
    "\n",
    "        # --- Zero pad if needed ---\n",
    "        target_len = int(self.seq_duration * sample_rate)\n",
    "        if audio_segment.shape[-1] < target_len:\n",
    "            pad_amount = target_len - audio_segment.shape[-1]\n",
    "            audio_segment = torch.nn.functional.pad(audio_segment, (0, pad_amount))\n",
    "\n",
    "        # --- Resample if needed ---\n",
    "        if sample_rate != self.resample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sample_rate, self.resample_rate)\n",
    "            audio_segment = resampler(audio_segment)\n",
    "\n",
    "        label = self.label_map[track.stroke_name]\n",
    "        return audio_segment, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.splits[self.set])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lM4Wk3tsRnqg"
   },
   "source": [
    "We initialize the dataset objects for train, validation, and test. We define the corresponding pytorch objects for data loading, defining the batch_size (paralellization on the GPU) and the num_workers ( data loading paralellization on CPU/memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1986,
     "status": "ok",
     "timestamp": 1673437894558,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "072HyKNfHs7q"
   },
   "outputs": [],
   "source": [
    "#### Pytorch dataset loaders\n",
    "train_dataset = MridangamDataset(mirdataset=mridangam,subset=0, random_seed=RANDOM_SEED)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=64,num_workers=2,pin_memory=True)\n",
    "valid_dataset = MridangamDataset(mirdataset=mridangam,subset=1, random_seed=RANDOM_SEED)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset,batch_size=64,num_workers=2,pin_memory=True)\n",
    "test_dataset = MridangamDataset(mirdataset=mridangam,subset=2, random_seed=RANDOM_SEED)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=64,num_workers=2,pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRowsZK3HfDi"
   },
   "source": [
    "\n",
    "**Which batch size/learning rate?**\n",
    "\n",
    "Theory suggests that when multiplying the batch size by k, one should multiply the learning rate by sqrt(k) to keep the variance in the gradient expectation constant. See page 5 at A. Krizhevsky. One weird trick for parallelizing convolutional neural networks: https://arxiv.org/abs/1404.5997\n",
    "\n",
    "However, recent experiments with large mini-batches suggest for a simpler linear scaling rule, i.e multiply your learning rate by k when using mini-batch size of kN. See P.Goyal et al.: Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour https://arxiv.org/abs/1706.02677"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Keh79hDpSUHG"
   },
   "source": [
    "## Training a pytorch lightning classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpwMhNSUSrRJ"
   },
   "source": [
    "We extend the pytorch lightning module according to the [documentation](https://lightning.ai/docs/pytorch/stable/model/train_model_basic.html). This may contain a definition of the layers in the neural network and how the data flows (how the layers are connected). You may overwrite other functions from `pl.LightningModule`, as described [here](https://lightning.ai/docs/pytorch/LTS/common/lightning_module.html). The most important are `training_step` and `configure_optimizers`, in which we define the training loss and the optimizers.\n",
    "\n",
    "W = W - lr * Delta(W) -> Stochastic gradient descent\n",
    "W = [w1 ... w10] [l1...l10] \n",
    "\n",
    "```\n",
    ">>> class LitModel(pl.LightningModule):\n",
    "...\n",
    "...     def __init__(self):\n",
    "...         super().__init__()\n",
    "...         self.l1 = torch.nn.Linear(28 * 28, 10)\n",
    "...\n",
    "...     def forward(self, x):\n",
    "...         return torch.relu(self.l1(x.view(x.size(0), -1)))\n",
    "...\n",
    "...     def training_step(self, batch, batch_idx):\n",
    "...         x, y = batch\n",
    "...         y_hat = self.forward(x)\n",
    "...         loss = F.cross_entropy(y_hat, y)\n",
    "...         return loss\n",
    "...\n",
    "...     def configure_optimizers(self):\n",
    "...         return torch.optim.Adam(self.parameters(), lr=0.02)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sHVW9hNsNgZ1"
   },
   "source": [
    "We first build a Conv2D module which stacks several layers: 2D convolution, batch normalization, relu, maxpool, dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1673437894561,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "UHdAsAM1Nfex"
   },
   "outputs": [],
   "source": [
    "class Conv_2d(torch.nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, shape=3, pooling=2, dropout=0.1):\n",
    "        super(Conv_2d, self).__init__()\n",
    "        self.conv = torch.nn.Conv2d(input_channels, output_channels, shape, padding=shape//2)\n",
    "        self.bn = torch.nn.BatchNorm2d(output_channels)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.maxpool = torch.nn.MaxPool2d(pooling)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, wav):\n",
    "        out = self.conv(wav)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = self.dropout(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRF81_4YZ4cf"
   },
   "source": [
    "We predict the 10 classes of the Mridangam stroke dataset with a CNN using as input the mel-spectrograms computed from audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 529,
     "status": "ok",
     "timestamp": 1673437965762,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "0V9lIsjDOCwZ"
   },
   "outputs": [],
   "source": [
    "class CNN(pl.LightningModule):\n",
    "    '''\n",
    "    Simple CNN classification architecture: https://music-classification.github.io/tutorial/part3_supervised/tutorial.html\n",
    "    '''\n",
    "    def __init__(self, num_channels=16, \n",
    "                       sample_rate=44100, \n",
    "                       n_fft=1024, \n",
    "                       f_min=0.0, \n",
    "                       f_max=11025.0, \n",
    "                       num_mels=128, \n",
    "                       num_classes=10):\n",
    "        super().__init__()\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "        self.celoss = torch.nn.CrossEntropyLoss()\n",
    "\t\t# mel spectrogram\n",
    "        self.melspec = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate, \n",
    "                                                            n_fft=n_fft, \n",
    "                                                            f_min=f_min, \n",
    "                                                            f_max=f_max, \n",
    "                                                            n_mels=num_mels)\n",
    "        self.amplitude_to_db = torchaudio.transforms.AmplitudeToDB()\n",
    "        self.input_bn = torch.nn.BatchNorm2d(1)\n",
    "\n",
    "        # convolutional layers\n",
    "        self.layer1 = Conv_2d(1, num_channels, pooling=(2, 2))\n",
    "        self.layer2 = Conv_2d(num_channels, num_channels, pooling=(2, 2))\n",
    "        self.layer3 = Conv_2d(num_channels, num_channels * 2, pooling=(2, 2))\n",
    "        self.layer4 = Conv_2d(num_channels * 2, num_channels * 2, pooling=(3, 1))\n",
    "        self.layer5 = Conv_2d(num_channels * 2, num_channels * 4, pooling=(3, 1))\n",
    "\n",
    "        # dense layers\n",
    "        self.dense1 = torch.nn.Linear(num_channels * 4, num_channels * 4)\n",
    "        self.dense_bn = torch.nn.BatchNorm1d(num_channels * 4)\n",
    "        self.dense2 = torch.nn.Linear(num_channels * 4, num_classes)\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "        #### metrics\n",
    "        self.train_acc = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes)\n",
    "        self.valid_acc = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes)\n",
    "        self.test_acc = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes)\n",
    "        self.test_cm = torchmetrics.classification.ConfusionMatrix(task='multiclass', num_classes=num_classes)\n",
    "\n",
    "    def forward(self, wav):\n",
    "        # input Preprocessing\n",
    "        out = self.melspec(wav)\n",
    "        out = self.amplitude_to_db(out)\n",
    "\n",
    "        # input batch normalization\n",
    "        out = self.input_bn(out)\n",
    "       \n",
    "        # convolutional layers\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        \n",
    "        # reshape\n",
    "        out = torch.flatten(out, 1)\n",
    "\n",
    "        # dense layers\n",
    "        out = self.dense1(out)\n",
    "        out = self.dense_bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.dense2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        waveform, label = batch\n",
    "        logits = self.forward(waveform)\n",
    "        loss = self.celoss(logits, label)\n",
    "        self.log('train_loss', loss)\n",
    "        _, pred = torch.max(logits, 1)\n",
    "        self.train_acc(pred, label)\n",
    "        self.log('train_acc', self.train_acc, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        waveform, label = batch\n",
    "        logits = self.forward(waveform)\n",
    "        # import pdb;pdb.set_trace()\n",
    "        loss = self.celoss(logits, label)\n",
    "        self.log('val_loss', loss)\n",
    "        _, pred = torch.max(logits, 1)\n",
    "        self.valid_acc(pred, label)\n",
    "        self.log('valid_acc', self.valid_acc, on_step=True, on_epoch=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        waveform, label = batch\n",
    "        b, c, t = waveform.size()\n",
    "        logits = self.forward(waveform)\n",
    "        loss = self.celoss(logits, label)\n",
    "        self.log('test_loss', loss)\n",
    "        _, pred = torch.max(logits, 1)\n",
    "        self.test_acc(pred, label)\n",
    "        self.log('test_acc', self.test_acc, on_step=True, on_epoch=True)\n",
    "        self.test_cm(pred, label)\n",
    "\n",
    "    def get_progress_bar_dict(self):\n",
    "        # don't show the version number\n",
    "        items = super().get_progress_bar_dict()\n",
    "        items.pop(\"v_num\", None)\n",
    "        return items\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3,weight_decay=0.0001)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)  # reduce the learning after 10 epochs by a factor of 10\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBgC2yQhZdFx"
   },
   "source": [
    "We train the model defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 694,
     "referenced_widgets": [
      "551a4f6f93a147c59405e8844d5ba253",
      "759235e439be40b08d096c5f2ca5f146",
      "e3465ac4bec74c5b98b3ca426de48f63",
      "e33d57e5ec894f00a96b9eeedb21a8ef",
      "7e594656cae24fa3844bf0f3108b30ee",
      "75379fd472994f62875e0cca31e8f793",
      "2ba8de221ad94db9960d7ef70a59b238",
      "4a0a8c1a3954408f81c00661c1aab7f1",
      "a42e31f2f13145a490c93a9ecda3882b",
      "73a01eee0ab94dcbba1980c5838c9fc1",
      "10141052e1904303920d14de4b529f29",
      "d2e989df85064ce4be299aa2ea7b825a",
      "f9f15b06ad524a34aa26ee5959589f47",
      "774a9bc413b54911aafb451a7c24d451",
      "7273a480060a433a8f615193162ed1e8",
      "c298fd05f6f84825b021ea88035cba58",
      "119fd5396d8b4f59a322a61056da2a1c",
      "f00879adbef04eb3a3eb45d72aa4435b",
      "f9821b9feaa249318edf4f38348f96e6",
      "b8ace1ac277e447fb0a712d4e4b099b2",
      "630c2216b51548ce96bb2e86caee0dc4",
      "1651bb5d8565420091e7624c14a2e4bf",
      "daa0f392e4f94cde91c360555b4c3a35",
      "10537e5e0bd94bd1854892776625755e",
      "4ce036068fce43e499b8ed4f3d341c2b",
      "72201e2db811492592669eddfd48f3f9",
      "686a96ccb83c450092b07a69007139cf",
      "7d7173d5e2c344b785b270811c9106b4",
      "e5c6824f4a154a51b08c2d0f11b9b106",
      "d9f7f704e59941eaa83e3f214a2ba7d6",
      "290d1632bdcf473ea34d72fb5c762b95",
      "eddb43c3b01b4224b248ac3ef48e2590",
      "26aee991f16f4fb4b890aad58bc617d4",
      "052ef73ed0be4897955af7ae755904a3",
      "eae094f6f4084bdba10bca8995f4d3d8",
      "7533f0c5afa74e08bf6e98a2975ff3fb",
      "c2c3ba0edbab478e87bcaaea7cc712e4",
      "4d0e0848620a44bbbc888e23e3be0860",
      "ff21f55760df4fa083e1fda0a375d5cc",
      "ad89016f694d4bd6b0a7e2ec5c2e4b66",
      "759cc05ea1004ceea11b1546a963f93a",
      "9b8f67cc34714d8682cc98b19515939e",
      "a8da05b240864441b5ff2dd5ee32b27c",
      "5858b022c50647ea8412d06e03a4bc94",
      "21c4b8de5e8c4936820d2c392fa3275f",
      "885c021cd26a4a30989858cee9c4579a",
      "c093f60760cc4b8d99c86cf7996568a1",
      "252c04521b27491e8a9277643410c0b0",
      "e9ddfca81f49471f90bd3c1357f4e248",
      "5f2b82d652ff4ff6b9a573cae67c4b00",
      "9e0e509dfe4d4750b0cc0f35fd48f20a",
      "e06ff15de9e742b78cf6f7fe21925ffc",
      "c42e326c49ad43918a205551b231e542",
      "591655e6a0a84761afaaf4fb88b5adcc",
      "1f6f6723df834f03b03ce54e15f50afc"
     ]
    },
    "executionInfo": {
     "elapsed": 44954,
     "status": "ok",
     "timestamp": 1673438064210,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "gzLG38D1H9XQ",
    "outputId": "754ffd04-4df7-4b6b-a8db-236948cb73b5"
   },
   "outputs": [],
   "source": [
    "#### Initialize the model\n",
    "model = CNN()\n",
    "\n",
    "#### Initialize a trainer\n",
    "trainer = pl.Trainer(max_epochs=3, accelerator=\"auto\")\n",
    "\n",
    "#### Train the model\n",
    "trainer.fit(model, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIg6fmr4Zj4c"
   },
   "source": [
    "Once the model is trained we can use it to process data, save it, get the metrics on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277,
     "referenced_widgets": [
      "72bf66dacf6e41daab3167b7e7ce1037",
      "0f44ac912cf54da99794e9dc0a3f1e11",
      "29fb3b6434af4ec680e3c4105c90077b",
      "dd46bcf957d94987b693f42cbae78e61",
      "2ae953e256a54fa4b3f0b61db7af7271",
      "f8139325465c4942bf35b3ae3e289db8",
      "955d35943f184934a5efd84f836dc370",
      "8dfee2ffc7ba455b918b5380526d6da5",
      "259c0e409b0743d2ad11e4d152c5f48b",
      "4bc52232414d4dcf8cb26c41f78ebb1c",
      "6240bb57efd447bdb6cedda8ffce7677"
     ]
    },
    "executionInfo": {
     "elapsed": 2929,
     "status": "ok",
     "timestamp": 1673438077284,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "sRfkS5lmj_4T",
    "outputId": "01c3038c-cb4f-4134-97f7-5d3f6ecfeb46"
   },
   "outputs": [],
   "source": [
    "#### Put the model in production\n",
    "model.eval()\n",
    "\n",
    "#### Compute metrics on the test set\n",
    "trainer.test(dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "executionInfo": {
     "elapsed": 299,
     "status": "ok",
     "timestamp": 1673438080951,
     "user": {
      "displayName": "Marius Miron",
      "userId": "15610443579159325826"
     },
     "user_tz": -60
    },
    "id": "4l9QTVHAIJcx",
    "outputId": "26c97aa0-213a-45cb-d32d-588677abf61a"
   },
   "outputs": [],
   "source": [
    "#### Compute confusion matrix on the test set\n",
    "confusion_matrix = model.test_cm.compute().cpu().numpy()\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.matshow(confusion_matrix)\n",
    "ax.set_xticks(range(len(train_dataset.label_map)))\n",
    "ax.set_yticks(range(len(train_dataset.label_map)))\n",
    "ax.set_xticklabels(train_dataset.label_map)\n",
    "ax.set_yticklabels(train_dataset.label_map)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
